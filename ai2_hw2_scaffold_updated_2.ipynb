{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "colab": {
      "name": "ai2_hw2_scaffold-updated-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi-xAklCxkPp"
      },
      "source": [
        "![](fig/univ.png)\n",
        "\n",
        "# AI-2: Convolutional Neural Network\n",
        "## Homework 2: Convolutional Neural Network\n",
        "\n",
        "**AI2 Cohort 1**<br/>\n",
        "**Univ.AI**<br/>\n",
        "**Instructor**: Pavlos Protopapas<br />\n",
        "**Maximum Score**: 100\n",
        "\n",
        "<hr style=\"height:2.4pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MHp3UchxkPp"
      },
      "source": [
        "### INSTRUCTIONS\n",
        "\n",
        "\n",
        "- This homework is a jupyter notebook. Download and work on it on your local machine.\n",
        "\n",
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropraite plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOdcLmuexkPp"
      },
      "source": [
        "### Names of the people who worked on this homework together\n",
        "#### /name here/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvt-FePXxkPp"
      },
      "source": [
        "#RUN THIS CELL\n",
        "import os\n",
        "import pathlib\n",
        "working_dir = pathlib.Path().absolute()\n",
        "# Uncomment the line below to help debug if the path to included images don't show\n",
        "#print(working_dir)\n",
        "os.chdir(working_dir)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mESEPn9xxkPq",
        "outputId": "ad8d4905-4808-40ad-d2a3-6b4c9647971e"
      },
      "source": [
        "# Please download the 2.1.0 version of tensorflow for this homework and also tf_keras_vis\n",
        "!pip3 -qq install tf_keras_vis \n",
        "!pip -qq install tensorflow==2.1.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 421.8MB 29kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.9MB 41.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEY44WepxkPq"
      },
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eb1v7gNJxkPq",
        "outputId": "7014b34e-bdee-415c-df5c-c176e96e8ebf"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLFd0MgUxkPq",
        "outputId": "6c0ba4d5-18a0-4670-bda0-b1dc68dfb64d"
      },
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(\"tensorflow version\", tf.__version__)\n",
        "print(\"keras version\", tf.keras.__version__)\n",
        "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "print(\"Devices:\", devices)\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"All Pysical Devices\", tf.config.list_physical_devices())\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/datac_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "tf.random.set_seed(2266)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version 2.1.0\n",
            "keras version 2.2.4-tf\n",
            "Eager Execution Enabled: True\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Number of replicas: 1\n",
            "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
            "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "All Pysical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_TJcLVkxkPq",
        "outputId": "85f9c228-c260-40f4-ce0e-56ea1fbe876e"
      },
      "source": [
        "# Restart runtime and run the below cell to check if you have the correct version\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj_O_hONxkPq"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import certifi\n",
        "import urllib3  # For handling https certificate verification \n",
        "import scipy.ndimage as ndimage\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2pjdEKaxkPq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pylab as plt \n",
        "from scipy.signal import convolve2d\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "#Some imports for getting the CIFAR-10 dataset and for help with visualization*]\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils import normalize\n",
        "from matplotlib import cm\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import os\n",
        "import certifi\n",
        "import urllib3  # For handling https certificate verification \n",
        "import scipy.ndimage as ndimage\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Please download the packages that are missing in your colab environment\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS8LGTcPxkPq"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n",
        "<h1> Overview </h1> \n",
        "\n",
        "<br />\n",
        "\n",
        "In this homework, we will explore Convolutional Neural Networks (CNNs).  We will begin by building a CNN to classify CIFAR-10 images, a standard pedagogical problem, and use saliency maps to understand what the network is paying attention to. We will then see that CNNs aren't just for classifying. They can serve as image input processing for a variety of tasks, as we will show by training a network to rotate faces upright.\n",
        "\n",
        "\n",
        "<h2> Part 1: Building a Basic CNN Model [60pts total] </h2>\n",
        "<br />\n",
        "\n",
        "In this question, you will use Keras to create a convolutional neural network for predicting the type of object shown in images from the [CIFAR-10](https://keras.io/datasets/#cifar10-small-image-classification) dataset, which contains 50,000 32x32 training images and 10,000 test images of the same size, with a total of 10 sizes.\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "<h4> Loading CIFAR-10 and Constructing the Model. </h4>\n",
        "<br />\n",
        "\n",
        "Load CIFAR-10 and use a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten Layers (not necessarily in this order, and you can use as many layers as you'd like) to build your classification model. You may use an existing architecture like AlexNet or VGG16, or create one of your own design. However, you should construct the network yourself and not use a pre-written implementation. At least one of your Conv2D layers should have at least 9 filters to be able to do question 1.3.\n",
        "<br /><br />\n",
        "\n",
        "Convolutional neural networks are computationally intensive. We highly recommend that you train your model on a system using GPUs (take a look at Google Colab's runtime settings for accessing a GPU environment free of cost). On CPUs, this training can take over an hour. On GPUs, it can be done within minutes. If you become frustrated having to rerun your model every time you open your notebook, take a look at how to save your model weights as explicitly detailed in **Part 2**, where it is required to save your weights.\n",
        "<br /><br />\n",
        "\n",
        "You can approach the problems in this question by first creating a model assigning 32 filters to each Conv2D layer recreate the model with 64 filters/layer, 128, etc. For each generated model, keep track of the total number of parameters.\n",
        "<br /><br />\n",
        "\n",
        "**1.1** [12pts] Report the total number of parameters in your model. How does the number of total parameters change (linearly, exponentially) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture, increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship.\n",
        "<br /><br />\n",
        "\n",
        "**1.2** [14pts total] Choose a model, train and evaluate it.\n",
        "<br /><br />\n",
        "\n",
        " **1.2.1** [10pts] Take your model from above and train it. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs.  Your validation accuracy should exceed 70%. Training for 10 epochs on a CPU should take about 30-60 minutes. \n",
        " <br /><br />\n",
        " \n",
        "**1.2.2** [4pts] Plot the loss and accuracy (both train and test) for your chosen architecture.\n",
        " <br /><br />\n",
        " \n",
        "*Techniques to Visualize the Model.*\n",
        " <br /><br />\n",
        " \n",
        "We will gain an intuition into how our model is processing the inputs in two ways.  First we'll ask you to use feature maps to visualize the activations in the intermediate layers of the network. We've provided a helper function `get_feature_maps` to aid in extracting feature maps from layer outputs in your model network.  Feel free to take advantage of it if you'd like.  We'll also ask you to use [saliency maps](https://arxiv.org/abs/1312.6034) to visualize the pixels that have the largest impact on the classification of an input (image in this case), as well as a more recent development,[Grad-CAM](https://arxiv.org/abs/1610.02391), which has been shown to better indicate the attention of CNNs.\n",
        " <br /><br />\n",
        " \n",
        "**1.3** [10pts] For a given input image from the test set that is correctly classified, use your model and extract 9 feature maps from an intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the feature maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_feature_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name.\n",
        " <br /><br />\n",
        " \n",
        "**1.4** [10pts] For the same input image generate and plot a (SmoothGrad) saliency map to show the pixels in the image most pertinent to classification, and a Grad-CAM heatmap. This is most easily done with the [tf-keras-vis](https://pypi.org/project/tf-keras-vis/) package. Take a look at the \"Usage\" examples; it will be straightforward to apply to our model. Feel free to pick your own [colormap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html); the `jet` colormap is harder to interpret than sequential ones. Arrange the three plots in a row using subplots: Original Image, Saliency Map, GradCAM. Which visualization is easier to understand in your case, and what does the network seem to be focusing on?\n",
        " <br /><br />\n",
        " \n",
        "**1.5** [14pts] Repeat `1.4` for an image from the test set that is *incorrectly classified*, indicating both the incorrect label and what the correct label should be, and from the visualizations of network attention, hypothesize why the network arrived at its answer. (Make sure you pass a new loss to the visualizers that uses the *incorrect* class index, because we want to see what caused the network to think the image was in that category!) If you had control over what images go in the training dataset, how could you modify it to avoid this particular network failure?\n",
        "\n",
        "\n",
        "\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJ9NPK7xkPq"
      },
      "source": [
        "**Helper code to generate feature maps**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dAbrxIuxkPq"
      },
      "source": [
        "def get_feature_maps(model, layer_id, input_image):\n",
        "    \"\"\"Returns intermediate output (activation map) from passing an image to the model\n",
        "    \n",
        "    Parameters:\n",
        "        model (tf.keras.Model): Model to examine\n",
        "        layer_id (int): Which layer's (from zero) output to return\n",
        "        input_image (ndarray): The input image\n",
        "    Returns:\n",
        "        maps (List[ndarray]): Feature map stack output by the specified layer\n",
        "    \"\"\"\n",
        "    model_ = Model(inputs=[model.input], outputs=[model.layers[layer_id].output])\n",
        "    return model_.predict(np.expand_dims(input_image, axis=0))[0,:,:,:].transpose((2,0,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZBss7TsxkPq"
      },
      "source": [
        "**A dictionary to turn class index into class labels for CIFAR-10**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmlfGb0_xkPq"
      },
      "source": [
        "cifar10dict = {0 : 'airplane', 1 : 'automobile', 2 : 'bird', 3 : 'cat', 4 : 'deer', 5 : 'dog', 6 : 'frog', 7 : 'horse', 8 : 'ship', 9 : 'truck'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wGdDcCxkPq"
      },
      "source": [
        "### 1.1\n",
        "\n",
        "\n",
        "**1.1 [12pts]** Report the total number of parameters in your model. How does the number of total parameters change (linearly, exponentially) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture, increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMvRp5j3xkPq"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL5totA-xkPq"
      },
      "source": [
        "### 1.2\n",
        "\n",
        "**1.2 Choosing a Model, Training and Evaluating It. [7pts total]**\n",
        "\n",
        "\n",
        " **[10pts]** Take your model from above and train it. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs.  Your validation accuracy should exceed 70%. Training for 10 epochs on a CPU should take about 30-60 minutes.\n",
        " \n",
        " **[4pts]** Plot the loss and accuracy (both train and test) for your chosen architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyD9I1i7xkPq"
      },
      "source": [
        "# your code here\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkwDI3MjxkPq"
      },
      "source": [
        "### 1.3\n",
        "\n",
        "\n",
        "**1.3 [10pts]** For a given input image from the test set that is correctly classified, use your model and extract 9 feature maps from an intermediate convolutional layer of your choice and plot the images in a 3x3 grid (use `imshow`'s `cmap='gray'` to show the feature maps in black & white).  Make sure to plot (and clearly label) your original input image as well. You may use the provided `get_feature_maps` function and the `cifar10dict` dictionary to convert class index to the correct class name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQqlpd-VxkPq"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wvS-asexkPq"
      },
      "source": [
        "### 1.4\n",
        "\n",
        "**1.4 [10pts]** For the same input image generate and plot a (SmoothGrad) saliency map to show the pixels in the image most pertinent to classification, and a Grad-CAM heatmap. This is most easily done with the [tf-keras-vis](https://pypi.org/project/tf-keras-vis/) package. Take a look at the \"Usage\" examples; it will be straightforward to apply to our model. Feel free to pick your own [colormap](https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html); the `jet` colormap is harder to interpret than sequential ones. Arrange the three plots in a row using subplots: Original Image, Saliency Map, GradCAM. Which visualization is easier to understand in your case, and what does the network seem to be focusing on?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKSsXAscxkPq"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHEUSW3OxkPq"
      },
      "source": [
        "### 1.5\n",
        "**[14pts]** Repeat *1.4* for an image from the test set that is *incorrectly classified*, indicating both the incorrect label and what the correct label should be, and from the visualizations of network attention, hypothesize why the network arrived at its answer. (Make sure you pass a new loss to the visualizers that uses the *incorrect* class index, because we want to see what caused the network to think the image was in that category!) If you had control over what images go in the training dataset, how could you modify it to avoid this particular network failure?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3AyiwbxkPq"
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvfFiBNAxkPq"
      },
      "source": [
        "<div class='exercise'> <b> Question 2: Image Orientation Estimation [30pts] </b></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH7Mh0pkxkPq"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n",
        "    \n",
        "<h2> Part 2: Regression with CNN [40pts total] </h2>\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "In this problem we will construct a neural network to predict *how far a face is from being \"upright\"*. \n",
        "\n",
        "Image orientation estimation with convolutional networks was first implemented in 2015 by Fischer, Dosovitskiy, and Brox in a paper titled [\"Image Orientation Estimation with Convolutional Networks\"](https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf), where the authors trained a network to straighten a wide variety of images using the Microsoft COCO dataset. \n",
        "\n",
        "In order to have a reasonable training time for a homework, we will be working on a subset of the problem where we just straighten images of faces. To do this, we will be using the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset of celebrity faces, where we assume that professional photographers have taken level pictures. \n",
        "\n",
        "The training will be supervised, with a rotated image (up to $\\pm 60^\\circ$) as an input, and the amount (in degrees) that the image has been rotated as a target. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmG1dhV4xkPq"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#EFF8D0\">\n",
        "    \n",
        "<h2> Questions </h2>\n",
        "\n",
        "### Data preparation [20 points]\n",
        "\n",
        "**2.1.1[10 points]** **Loading CelebA and Thinking about Datasets**.\n",
        "\n",
        "Run the cells provided to automatically download the CelebA dataset. It is about 1.3GB, which can take 10-20 minutes to download. This happens only once; in the future when you rerun the cell, it will use the dataset stored on your google drive.\n",
        "\n",
        "**NOTE**: If you get a `NonMatchingChecksumError`, note that this is a documented issue and is because of multiple server requests. Refer [here](https://github.com/tensorflow/datasets/issues/1482) for more details.\n",
        "\n",
        "The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n",
        "\n",
        "[TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from part 1 where the entire dataset was loaded in as an array. \n",
        "\n",
        "Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`). \n",
        "\n",
        "Explain in less than 150 words why using this approach is advantageous over loading the entire data in one array.\n",
        "\n",
        "   \n",
        "**2.1.2[5 points]** **Taking a look**.\n",
        "\n",
        "In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. \n",
        "\n",
        "Hint: one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more.\n",
        "\n",
        "**2.1.3[5 points]** **Conceptual Question**\n",
        "\n",
        "Dropout layers have been shown to work well for regularizing deep neural networks, and can be used for very little computational cost. \n",
        "\n",
        "Write in **3-5 sentences** if it is a good idea to use dropout layers? \n",
        "\n",
        "Explain, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model.\n",
        "\n",
        "### Building and training your CNN [20 points]\n",
        "\n",
        "**2.2.1[5 points]** **Compiling your model**.\n",
        "\n",
        "Construct a model with multiple Conv layers and any other layers you think would help. Be sure to output `<yourmodelname>.summary()` as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we were able to it with substantially fewer. Any working setup is acceptable though.\n",
        "\n",
        "**2.2.2[10 points]** **Training your model**.\n",
        "\n",
        "Train your model using `<yourmodelname>.fit()`. The syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. Your final model should be trained on all the available training data though. You should achieve a validation loss of less than 9, corresponding to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n",
        "\n",
        "**2.2.3[5 points]** **Evaluating your model**.\n",
        "\n",
        "Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like this:\n",
        "\n",
        "This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n",
        "\n",
        "### Further Analysis [10 points]\n",
        "\n",
        "**2.3.1[8 points]** **Visualizing Attention**.\n",
        "\n",
        "Like in part 1, we will use the saliency map and GradCAM to see what the network was looking at to determine the orientation of a testset image. \n",
        "\n",
        "The code will be very similar to what you used in part 1, but there are two important modifications. \n",
        "\n",
        "In defining the new `model_modifier(m)` function, simply replace the contents with `pass`. This is because your model does not (should not) have a softmax activation on the last layer, so we don't need this function to do anything. \n",
        "\n",
        "The other modification is to change the loss function (that was defined as a Python lambda function) to an MSE, so it should now be `tf.keras.backend.mean((output - label)**2)` where label is the actual rotation of the image. Pick any image from the test set, and like before, make a row of 3 subplots showing the original image, the saliency map, and the GradCAM output. \n",
        "\n",
        "Answer in 1-2 lines what types of features does the network appear to use to determine orientation?\n",
        "\n",
        "**2.3.2[2 points]** **Correct an image of your choosing**.\n",
        "\n",
        "Find an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_IllAC7xkPq"
      },
      "source": [
        "**2.1.1** **Loading CelebA and Thinking about Datasets**.\n",
        "\n",
        "Run the cells provided to automatically download and load the CelebA dataset. It is about 1.3GB, and may take some time to download. Please ensure you are running the `2.1.0` version of tensorflow.\n",
        "\n",
        "The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and test dataset `test_rot_ds`. \n",
        "\n",
        "[TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from part 1 where the entire dataset was loaded in as an array. \n",
        "\n",
        "Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMk3BYicxkPq",
        "outputId": "d7e05715-401e-4d41-ae4d-e046ed41defd"
      },
      "source": [
        "#mount your own drive to avoid downloading the data multiple time \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhN-9XIKxkPq",
        "outputId": "6e02581a-f5f5-4ec5-942d-559de800dfc7"
      },
      "source": [
        "#Following creates a directory, downloads the file and unzips it. \n",
        "\n",
        "if os.path.isdir('gdrive/My Drive/celeb_a/2.0.1'):\n",
        "    print('Found dataset' )\n",
        "else:\n",
        "    os.makedirs('gdrive/My Drive/celeb_a')\n",
        "    !gdown -qq https://drive.google.com/u/2/uc?id=1CdfrT4f87b8ggx02TxsBuMTu0bqSIpNX&export=download\n",
        "    !unzip 2.0.1.zip  -d gdrive/My\\ Drive/celeb_a/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMWNIknxxkPq"
      },
      "source": [
        "# This command will use the celeb_a dataset that you downloaded, and load it into train and test 'tensorflow.Datasets'\n",
        "\n",
        "train_celeb, test_celeb = tfds.load('celeb_a', split=['train', 'test'], shuffle_files=False, data_dir = 'gdrive/My Drive/', download=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZikj0GWxkPq"
      },
      "source": [
        "# You may use the following two functions\n",
        "def normalize_image(img):\n",
        "    return tf.cast(img, tf.float32)/255.\n",
        "\n",
        "def rot_resize(img, deg):\n",
        "    rotimg = ndimage.rotate(img, deg, reshape=False, order=3)\n",
        "    rotimg = np.clip(rotimg, 0., 1.)\n",
        "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
        "    return rotimg\n",
        "\n",
        "################################################################\n",
        "# Don't manually invoke these functions; they are for Dataset \n",
        "# pipelining that is already done for you.\n",
        "################################################################\n",
        "def tf_rot_resize(img, deg):\n",
        "    \"\"\"Dataset pipe that rotates an image and resizes it to 140x120\"\"\"\n",
        "    rotimg = tfa.image.rotate(img, deg/180.*np.pi, interpolation=\"BILINEAR\")\n",
        "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
        "    return rotimg\n",
        "\n",
        "def tf_random_rotate_helper(image):\n",
        "    \"\"\"Dataset pipe that normalizes image to [0.,1.] and rotates by a random\n",
        "    amount of degrees in [-60.,60.], returning an (input,target) pair consisting\n",
        "    of the rotated and resized image and the degrees it has been rotated by.\"\"\"\n",
        "    image = normalize_image(image)\n",
        "    deg = tf.random.uniform([],-60.,60.)\n",
        "    return (tf_rot_resize(image,deg), deg)  # (data, label)\n",
        "\n",
        "def tf_random_rotate_image(element):\n",
        "    \"\"\"Given an element drawn from the CelebA dataset, this returns a rotated\n",
        "    image and the amount it has been rotated by, in degrees.\"\"\"\n",
        "    image = element['image']\n",
        "    image, label = tf_random_rotate_helper(image)\n",
        "    image.set_shape((140,120,3))\n",
        "    return image, label\n",
        "################################################################"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHD1TcShxkPq"
      },
      "source": [
        "# Pipeline for creating randomly rotated images with their target labels being \n",
        "# the amount they were rotated, in degrees.\n",
        "train_rot_ds = train_celeb.map(tf_random_rotate_image)\n",
        "test_rot_ds = test_celeb.map(tf_random_rotate_image)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_RWdEMFxkPq"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7bPVVQ6xkPq"
      },
      "source": [
        "**2.1.2** **Taking a look**.\n",
        "\n",
        "In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. \n",
        "\n",
        "Hint: one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE3qeD-0xkPq"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZkGWfeMxkPq"
      },
      "source": [
        "**2.1.3** **Conceptual Question**\n",
        "\n",
        "Dropout layers have been shown to work well for regularizing deep neural networks, and can be used for very little computational cost. \n",
        "\n",
        "Write in **3-5 sentences** if it is a good idea to use dropout layers? \n",
        "\n",
        "Explain, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKmN5xkrxkPq"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGq_Zt0ZxkPq"
      },
      "source": [
        "**2.2.1** **Compiling your model**.\n",
        "\n",
        "Construct a model with multiple Conv layers and any other layers you think would help. Be sure to output `<yourmodelname>.summary()` as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we were able to it with substantially fewer. Any working setup is acceptable though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94yDh5Vnz8Qs"
      },
      "source": [
        "from tensorflow import keras"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvJdBIcnxkPq"
      },
      "source": [
        "from functools import partial\n",
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,padding=\"SAME\", use_bias=False)\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "          DefaultConv2D(filters, strides=strides),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          self.activation,\n",
        "          DefaultConv2D(filters),\n",
        "            \n",
        "          keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "              DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "              keras.layers.BatchNormalization()]\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V__czyiX0p27",
        "outputId": "5e7ae486-cf80-49f6-e5c5-244b5df16c0c"
      },
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(DefaultConv2D(64, kernel_size=7, strides=2,input_shape=[140, 120, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "model.add(keras.layers.Dense(1024,activation='elu',kernel_initializer='he_normal'))\n",
        "model.add(keras.layers.Dense(1, activation=\"linear\"))    \n",
        "model.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_108 (Conv2D)          (None, 70, 60, 64)        9408      \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 70, 60, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 70, 60, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 35, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "residual_unit_48 (ResidualUn (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_49 (ResidualUn (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_50 (ResidualUn (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_51 (ResidualUn (None, 18, 15, 128)       230912    \n",
            "_________________________________________________________________\n",
            "residual_unit_52 (ResidualUn (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_53 (ResidualUn (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_54 (ResidualUn (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_55 (ResidualUn (None, 9, 8, 256)         920576    \n",
            "_________________________________________________________________\n",
            "residual_unit_56 (ResidualUn (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_57 (ResidualUn (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_58 (ResidualUn (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_59 (ResidualUn (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_60 (ResidualUn (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_61 (ResidualUn (None, 5, 4, 512)         3676160   \n",
            "_________________________________________________________________\n",
            "residual_unit_62 (ResidualUn (None, 5, 4, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "residual_unit_63 (ResidualUn (None, 5, 4, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 21,828,033\n",
            "Trainable params: 21,811,009\n",
            "Non-trainable params: 17,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SsllGHHxkPq"
      },
      "source": [
        "**2.2.2** **Training your model**.\n",
        "\n",
        "Train your model using `<yourmodelname>.fit()`. The syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. Your final model should be trained on all the available training data though. You should achieve a validation loss of less than 9, corresponding to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVEgSW7d8BZw"
      },
      "source": [
        "model.compile(loss='MSE',optimizer=keras.optimizers.Adam(lr=0.001),metrics=['mean_squared_error'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NcCc90BxkPq",
        "outputId": "a4ecb11f-8b0e-447f-a9b7-154387020aed"
      },
      "source": [
        "history = model.fit(train_rot_ds.batch(32), epochs=10, validation_data=test_rot_ds.batch(32), steps_per_epoch=128, validation_steps=128)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 128 steps, validate for 128 steps\n",
            "Epoch 1/10\n",
            "128/128 [==============================] - 35s 275ms/step - loss: 362.1877 - mean_squared_error: 362.1877 - val_loss: 1905.1449 - val_mean_squared_error: 1905.1453\n",
            "Epoch 2/10\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 101.6859 - mean_squared_error: 101.6859 - val_loss: 492.6295 - val_mean_squared_error: 492.6293\n",
            "Epoch 3/10\n",
            "128/128 [==============================] - 33s 258ms/step - loss: 63.2018 - mean_squared_error: 63.2018 - val_loss: 45.1410 - val_mean_squared_error: 45.1410\n",
            "Epoch 4/10\n",
            "128/128 [==============================] - 32s 254ms/step - loss: 48.2117 - mean_squared_error: 48.2117 - val_loss: 454.9502 - val_mean_squared_error: 454.9502\n",
            "Epoch 5/10\n",
            "128/128 [==============================] - 32s 253ms/step - loss: 47.4627 - mean_squared_error: 47.4627 - val_loss: 30.0565 - val_mean_squared_error: 30.0565\n",
            "Epoch 6/10\n",
            "128/128 [==============================] - 32s 250ms/step - loss: 43.1316 - mean_squared_error: 43.1316 - val_loss: 61.8930 - val_mean_squared_error: 61.8930\n",
            "Epoch 7/10\n",
            "128/128 [==============================] - 32s 251ms/step - loss: 37.2092 - mean_squared_error: 37.2092 - val_loss: 25.5099 - val_mean_squared_error: 25.5099\n",
            "Epoch 8/10\n",
            "128/128 [==============================] - 33s 255ms/step - loss: 26.5922 - mean_squared_error: 26.5922 - val_loss: 70.2426 - val_mean_squared_error: 70.2426\n",
            "Epoch 9/10\n",
            "128/128 [==============================] - 32s 250ms/step - loss: 14.3931 - mean_squared_error: 14.3931 - val_loss: 24.2973 - val_mean_squared_error: 24.2973\n",
            "Epoch 10/10\n",
            "128/128 [==============================] - 32s 251ms/step - loss: 22.4019 - mean_squared_error: 22.4019 - val_loss: 25.5322 - val_mean_squared_error: 25.5322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "y4ueYrid4CTd",
        "outputId": "f7aac636-0fc9-4554-e77f-470762a9a491"
      },
      "source": [
        "history = model.fit(train_rot_ds.batch(32), initial_epoch=10, epochs=20, validation_data=test_rot_ds.batch(32), steps_per_epoch=128, validation_steps=128)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 128 steps, validate for 128 steps\n",
            "Epoch 11/20\n",
            "128/128 [==============================] - 33s 256ms/step - loss: 15.3221 - mean_squared_error: 15.3221 - val_loss: 25.7966 - val_mean_squared_error: 25.7966\n",
            "Epoch 12/20\n",
            "128/128 [==============================] - 32s 253ms/step - loss: 15.0614 - mean_squared_error: 15.0614 - val_loss: 17.9405 - val_mean_squared_error: 17.9405\n",
            "Epoch 13/20\n",
            "128/128 [==============================] - 32s 253ms/step - loss: 13.7915 - mean_squared_error: 13.7915 - val_loss: 9.2971 - val_mean_squared_error: 9.2971\n",
            "Epoch 14/20\n",
            " 18/128 [===>..........................] - ETA: 14s - loss: 6.7463 - mean_squared_error: 6.7463"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-7e82925598ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_rot_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_rot_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOz0RYZzxkPq"
      },
      "source": [
        "**2.2.3** **Evaluating your model**.\n",
        "\n",
        "Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like this:\n",
        "\n",
        "This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVFZMFBDDwAG"
      },
      "source": [
        "model.save_weights('model.h5')"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lmf7Anj39PaU"
      },
      "source": [
        "mode.load_weights('model.h5')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2-cHLo0xkPq",
        "outputId": "43026af0-7e20-4a75-f943-4fa91240b647"
      },
      "source": [
        "model.evaluate(test_rot_ds.batch(32),steps=128)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128/128 [==============================] - 18s 137ms/step - loss: 7.9030 - mean_squared_error: 7.9030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.903047626838088, 7.9030466]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqmj1JtaxkPq"
      },
      "source": [
        "**2.3.1** **Visualizing Attention**.\n",
        "\n",
        "Like in part 1, we will use the saliency map and GradCAM to see what the network was looking at to determine the orientation of a testset image. \n",
        "\n",
        "The code will be very similar to what you used in part 1, but there are two important modifications. \n",
        "\n",
        "In defining the new `model_modifier(m)` function, simply replace the contents with `pass`. This is because your model does not (should not) have a softmax activation on the last layer, so we don't need this function to do anything. \n",
        "\n",
        "The other modification is to change the loss function (that was defined as a Python lambda function) to an MSE, so it should now be `tf.keras.backend.mean((output - label)**2)` where label is the actual rotation of the image. Pick any image from the test set, and like before, make a row of 3 subplots showing the original image, the saliency map, and the GradCAM output. \n",
        "\n",
        "Answer in 1-2 lines what types of features does the network appear to use to determine orientation?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_GjmDjTxkPq",
        "outputId": "39b4c8ac-6c57-4f31-ae43-3109f6a6f94f"
      },
      "source": [
        "mode = keras.models.Sequential()\n",
        "mode.add(DefaultConv2D(64, kernel_size=7, strides=2,input_shape=[140, 120, 3]))\n",
        "mode.add(keras.layers.BatchNormalization())\n",
        "mode.add(keras.layers.Activation(\"relu\"))\n",
        "mode.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    mode.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "mode.add(keras.layers.GlobalAvgPool2D())\n",
        "mode.add(keras.layers.Flatten())\n",
        "\n",
        "mode.add(keras.layers.Dense(1024,activation='elu',kernel_initializer='he_normal'))\n",
        "mode.add(keras.layers.Dense(1, activation=\"linear\"))    \n",
        "mode.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_252 (Conv2D)          (None, 70, 60, 64)        9408      \n",
            "_________________________________________________________________\n",
            "batch_normalization_252 (Bat (None, 70, 60, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 70, 60, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 35, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "residual_unit_112 (ResidualU (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_113 (ResidualU (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_114 (ResidualU (None, 35, 30, 64)        74240     \n",
            "_________________________________________________________________\n",
            "residual_unit_115 (ResidualU (None, 18, 15, 128)       230912    \n",
            "_________________________________________________________________\n",
            "residual_unit_116 (ResidualU (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_117 (ResidualU (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_118 (ResidualU (None, 18, 15, 128)       295936    \n",
            "_________________________________________________________________\n",
            "residual_unit_119 (ResidualU (None, 9, 8, 256)         920576    \n",
            "_________________________________________________________________\n",
            "residual_unit_120 (ResidualU (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_121 (ResidualU (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_122 (ResidualU (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_123 (ResidualU (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_124 (ResidualU (None, 9, 8, 256)         1181696   \n",
            "_________________________________________________________________\n",
            "residual_unit_125 (ResidualU (None, 5, 4, 512)         3676160   \n",
            "_________________________________________________________________\n",
            "residual_unit_126 (ResidualU (None, 5, 4, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "residual_unit_127 (ResidualU (None, 5, 4, 512)         4722688   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_7 ( (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 1025      \n",
            "=================================================================\n",
            "Total params: 21,828,033\n",
            "Trainable params: 21,811,009\n",
            "Non-trainable params: 17,024\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eojS03wHF2I_"
      },
      "source": [
        "mode.compile(loss='MSE',optimizer=keras.optimizers.Adam(lr=0.001),metrics=['mean_squared_error'])"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M_yvD_lGvKx",
        "outputId": "cf43c7a0-725f-4278-cb3f-e11de5e108d1"
      },
      "source": [
        "mode.evaluate(test_rot_ds.batch(32),steps=128)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128/128 [==============================] - 16s 128ms/step - loss: 7.9030 - mean_squared_error: 7.9030\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.903047626838088, 7.9030466]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhG0Y67AxkPq"
      },
      "source": [
        "*Your answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVb000WxkPq"
      },
      "source": [
        "**2.3.2** **Correct an image of your choosing**.\n",
        "\n",
        "Find an image or image(s) (not from the provided test/training sets), or make your own. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_PFui97xkPq"
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}